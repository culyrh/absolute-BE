"""
í†µí•© ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸
ëª¨ë“  ì¶”ì²œ ì•Œê³ ë¦¬ì¦˜(ê¸°ë³¸ + ML)ì„ í•œë²ˆì— ë¹„êµ í…ŒìŠ¤íŠ¸

ì‚¬ìš©ë²•:
    python -m app.comparison.benchmark
"""

import pandas as pd
import numpy as np
from typing import Dict, List
from datetime import datetime
import time
from pathlib import Path

from app.utils.data_loader import load_all_data
from app.comparison.algorithms.cosine_similarity import CosineSimilarityAlgorithm
from app.comparison.algorithms.euclidean_distance import EuclideanDistanceAlgorithm
from app.comparison.algorithms.pearson_correlation import PearsonCorrelationAlgorithm
from app.comparison.algorithms.popularity import PopularityAlgorithm
from app.comparison.algorithms.collaborative import CollaborativeAlgorithm
from app.comparison.algorithms.ahp_topsis import AHPTopsisAlgorithm
from app.services.ml_location_recommender import MLLocationRecommender


class BenchmarkTest:
    """í†µí•© ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.data = None
        self.train_data = None
        self.test_data = None
        self.centroids = None
        self.norm_cols = None
        self.ml_recommender = None
        self.results = {}
        
    def load_data(self):
        """ë°ì´í„° ë¡œë“œ"""
        print("="*80)
        print("ğŸ“Š ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸ - ë°ì´í„° ë¡œë”©")
        print("="*80 + "\n")
        
        # ëª¨ë“  ë°ì´í„° ë¡œë“œ
        self.data = load_all_data()
        
        # Train ë°ì´í„°
        self.train_data = self.data["recommend_result"]
        print(f"âœ… Train ë°ì´í„° ë¡œë“œ: {len(self.train_data)}ê°œ")
        
        # ì„¼íŠ¸ë¡œì´ë“œ ë°ì´í„°
        self.centroids = self.data["centroid"]
        
        # ì •ê·œí™” ì»¬ëŸ¼
        feature_cols = ["ì¸êµ¬[ëª…]", "êµí†µëŸ‰", "ìˆ™ë°•ì—…ì†Œ(ê´€ê´‘ì§€ìˆ˜)", "ìƒê¶Œë°€ì§‘ë„(ë¹„ìœ¨)", "ê³µì‹œì§€ê°€(í† ì§€ë‹¨ê°€)"]
        self.norm_cols = [f"{col}_norm" for col in feature_cols]
        
    def load_test_data(self, test_file_path: str = "data/test_data.csv"):
        """í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ"""
        print(f"\nğŸ“‚ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ì¤‘: {test_file_path}")
        
        try:
            self.test_data = pd.read_csv(test_file_path, encoding="utf-8-sig")
            print(f"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(self.test_data)}ê°œ")
            
            if "ëŒ€ë¶„ë¥˜" in self.test_data.columns:
                print(f"   - ëŒ€ë¶„ë¥˜ ì¢…ë¥˜: {self.test_data['ëŒ€ë¶„ë¥˜'].nunique()}ê°œ")
                print(f"   - ëŒ€ë¶„ë¥˜ ë¶„í¬:\n{self.test_data['ëŒ€ë¶„ë¥˜'].value_counts()}")
            
            return self.test_data
            
        except FileNotFoundError:
            print(f"âŒ ì˜¤ë¥˜: í…ŒìŠ¤íŠ¸ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {test_file_path}")
            raise
        except Exception as e:
            print(f"âŒ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            raise
    
    def initialize_ml(self):
        """ML ëª¨ë¸ ì´ˆê¸°í™” ë° í•™ìŠµ"""
        print("\n" + "="*80)
        print("ğŸ¤– ML ëª¨ë¸ í•™ìŠµ")
        print("="*80 + "\n")
        
        self.ml_recommender = MLLocationRecommender()
        
        start_time = time.time()
        accuracy = self.ml_recommender.train()
        train_time = time.time() - start_time
        
        print(f"âœ… ML í•™ìŠµ ì™„ë£Œ: ì •í™•ë„ {accuracy:.3f}, ì†Œìš” ì‹œê°„ {train_time:.2f}ì´ˆ")
        
        return accuracy
    
    def run_traditional_algorithm_test(self, algorithm, algorithm_name: str) -> Dict:
        """ì „í†µì ì¸ ì•Œê³ ë¦¬ì¦˜ í…ŒìŠ¤íŠ¸"""
        print(f"\nâ±ï¸  {algorithm_name} í…ŒìŠ¤íŠ¸ ì¤‘...")
        
        results = {
            "algorithm": algorithm_name,
            "type": "traditional",
            "top1_correct": 0,
            "top3_correct": 0,
            "top5_correct": 0,
            "total": len(self.test_data),
            "execution_times": [],
            "usage_type_accuracy": {}
        }
        
        for idx, row in self.test_data.iterrows():
            true_label = row["ëŒ€ë¶„ë¥˜"]
            test_df = pd.DataFrame([row])
            
            start_time = time.time()
            try:
                recommendations = algorithm.recommend(test_df, top_k=5)
                execution_time = time.time() - start_time
                results["execution_times"].append(execution_time)
                
                if len(recommendations) > 0:
                    # Top-1
                    if recommendations[0]["usage_type"] == true_label:
                        results["top1_correct"] += 1
                    
                    # Top-3
                    top3_types = [r["usage_type"] for r in recommendations[:3]]
                    if true_label in top3_types:
                        results["top3_correct"] += 1
                    
                    # Top-5
                    top5_types = [r["usage_type"] for r in recommendations[:5]]
                    if true_label in top5_types:
                        results["top5_correct"] += 1
                    
                    # ëŒ€ë¶„ë¥˜ë³„
                    if true_label not in results["usage_type_accuracy"]:
                        results["usage_type_accuracy"][true_label] = {"correct": 0, "total": 0}
                    
                    results["usage_type_accuracy"][true_label]["total"] += 1
                    if recommendations[0]["usage_type"] == true_label:
                        results["usage_type_accuracy"][true_label]["correct"] += 1
                        
            except Exception as e:
                print(f"  âŒ ì˜¤ë¥˜ (ì¸ë±ìŠ¤ {idx}): {str(e)}")
                results["execution_times"].append(0)
        
        # ì •í™•ë„ ê³„ì‚°
        results["top1_accuracy"] = (results["top1_correct"] / results["total"]) * 100 if results["total"] > 0 else 0
        results["top3_accuracy"] = (results["top3_correct"] / results["total"]) * 100 if results["total"] > 0 else 0
        results["top5_accuracy"] = (results["top5_correct"] / results["total"]) * 100 if results["total"] > 0 else 0
        results["avg_execution_time"] = np.mean(results["execution_times"]) * 1000 if results["execution_times"] else 0
        
        print(f"   âœ… Top-1: {results['top1_accuracy']:.2f}% | Top-3: {results['top3_accuracy']:.2f}% | Top-5: {results['top5_accuracy']:.2f}%")
        print(f"   âš¡ í‰ê·  ì‹¤í–‰ì‹œê°„: {results['avg_execution_time']:.2f} ms")
        
        return results
    
    def run_ml_test(self) -> Dict:
        """ML ì•Œê³ ë¦¬ì¦˜ í…ŒìŠ¤íŠ¸"""
        print(f"\nâ±ï¸  ML (Random Forest) í…ŒìŠ¤íŠ¸ ì¤‘...")
        
        results = {
            "algorithm": "ML (Random Forest)",
            "type": "ml",
            "top1_correct": 0,
            "top3_correct": 0,
            "top5_correct": 0,
            "total": len(self.test_data),
            "execution_times": [],
            "usage_type_accuracy": {}
        }
        
        for idx, row in self.test_data.iterrows():
            true_label = row["ëŒ€ë¶„ë¥˜"]
            
            start_time = time.time()
            try:
                predictions = self.ml_recommender._predict_from_row(row, top_n=5)
                execution_time = time.time() - start_time
                results["execution_times"].append(execution_time)
                
                if len(predictions) > 0:
                    # Top-1
                    if predictions[0]["category"] == true_label:
                        results["top1_correct"] += 1
                    
                    # Top-3
                    top3_types = [p["category"] for p in predictions[:3]]
                    if true_label in top3_types:
                        results["top3_correct"] += 1
                    
                    # Top-5
                    top5_types = [p["category"] for p in predictions[:5]]
                    if true_label in top5_types:
                        results["top5_correct"] += 1
                    
                    # ëŒ€ë¶„ë¥˜ë³„
                    if true_label not in results["usage_type_accuracy"]:
                        results["usage_type_accuracy"][true_label] = {"correct": 0, "total": 0}
                    
                    results["usage_type_accuracy"][true_label]["total"] += 1
                    if predictions[0]["category"] == true_label:
                        results["usage_type_accuracy"][true_label]["correct"] += 1
                        
            except Exception as e:
                print(f"  âŒ ì˜¤ë¥˜ (ì¸ë±ìŠ¤ {idx}): {str(e)}")
                results["execution_times"].append(0)
        
        # ì •í™•ë„ ê³„ì‚°
        results["top1_accuracy"] = (results["top1_correct"] / results["total"]) * 100 if results["total"] > 0 else 0
        results["top3_accuracy"] = (results["top3_correct"] / results["total"]) * 100 if results["total"] > 0 else 0
        results["top5_accuracy"] = (results["top5_correct"] / results["total"]) * 100 if results["total"] > 0 else 0
        results["avg_execution_time"] = np.mean(results["execution_times"]) * 1000 if results["execution_times"] else 0
        
        print(f"   âœ… Top-1: {results['top1_accuracy']:.2f}% | Top-3: {results['top3_accuracy']:.2f}% | Top-5: {results['top5_accuracy']:.2f}%")
        print(f"   âš¡ í‰ê·  ì‹¤í–‰ì‹œê°„: {results['avg_execution_time']:.2f} ms")
        
        return results
    
    def run_all_tests(self):
        """ëª¨ë“  ì•Œê³ ë¦¬ì¦˜ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸"""
        print("\n" + "="*80)
        print("ğŸš€ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        print("="*80)
        
        # ì „í†µì ì¸ ì•Œê³ ë¦¬ì¦˜ë“¤
        traditional_algorithms = {
            "ì½”ì‚¬ì¸ ìœ ì‚¬ë„": CosineSimilarityAlgorithm(self.centroids, self.norm_cols),
            "ìœ í´ë¦¬ë“œ ê±°ë¦¬": EuclideanDistanceAlgorithm(self.centroids, self.norm_cols),
            "í”¼ì–´ìŠ¨ ìƒê´€ê³„ìˆ˜": PearsonCorrelationAlgorithm(self.centroids, self.norm_cols),
            "ì¸ê¸°ë„ ê¸°ë°˜": PopularityAlgorithm(self.centroids, self.norm_cols, self.train_data),
            "í˜‘ì—… í•„í„°ë§": CollaborativeAlgorithm(self.centroids, self.norm_cols, self.train_data),
            "AHP-TOPSIS": AHPTopsisAlgorithm(self.centroids, self.norm_cols, self.train_data),
        }
        
        # ì „í†µì ì¸ ì•Œê³ ë¦¬ì¦˜ í…ŒìŠ¤íŠ¸
        for name, algorithm in traditional_algorithms.items():
            result = self.run_traditional_algorithm_test(algorithm, name)
            self.results[name] = result
        
        # ML ì•Œê³ ë¦¬ì¦˜ í…ŒìŠ¤íŠ¸
        if self.ml_recommender:
            result = self.run_ml_test()
            self.results["ML (Random Forest)"] = result
        
        # ê²°ê³¼ ì¶œë ¥
        self.print_results()
        
        # ê²°ê³¼ ì €ì¥
        self.save_results()
    
    def print_results(self):
        """ê²°ê³¼ ì¶œë ¥"""
        print("\n" + "="*80)
        print("ğŸ“Š ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ - ì „ì²´ ì•Œê³ ë¦¬ì¦˜ ë¹„êµ")
        print("="*80 + "\n")
        
        # í…Œì´ë¸” í—¤ë”
        print(f"{'ì•Œê³ ë¦¬ì¦˜':<25} {'ìœ í˜•':<12} {'Top-1':<10} {'Top-3':<10} {'Top-5':<10} {'ì‹¤í–‰ì‹œê°„(ms)':<15}")
        print("-" * 80)
        
        # ê²°ê³¼ ì¶œë ¥
        for name, result in self.results.items():
            algo_type = "ML" if result.get("type") == "ml" else "Traditional"
            print(f"{name:<25} {algo_type:<12} {result['top1_accuracy']:>6.2f}%  {result['top3_accuracy']:>6.2f}%  "
                  f"{result['top5_accuracy']:>6.2f}%  {result['avg_execution_time']:>10.2f}")
        
        print("\n" + "="*80)
        
        # ìµœê³  ì„±ëŠ¥ ë¶„ì„
        if self.results:
            # Top-1 ê¸°ì¤€ ìµœê³ 
            best_top1 = max(self.results.items(), key=lambda x: x[1]["top1_accuracy"])
            print(f"ğŸ¥‡ Top-1 ìµœê³ : {best_top1[0]} ({best_top1[1]['top1_accuracy']:.2f}%)")
            
            # Top-3 ê¸°ì¤€ ìµœê³ 
            best_top3 = max(self.results.items(), key=lambda x: x[1]["top3_accuracy"])
            print(f"ğŸ¥ˆ Top-3 ìµœê³ : {best_top3[0]} ({best_top3[1]['top3_accuracy']:.2f}%)")
            
            # ê°€ì¥ ë¹ ë¥¸ ì•Œê³ ë¦¬ì¦˜
            fastest = min(self.results.items(), key=lambda x: x[1]["avg_execution_time"])
            print(f"âš¡ ê°€ì¥ ë¹ ë¦„: {fastest[0]} ({fastest[1]['avg_execution_time']:.2f} ms)")
        
        print("="*80 + "\n")
    
    def save_results(self):
        """ê²°ê³¼ ì €ì¥"""
        output_dir = Path("test_results")
        output_dir.mkdir(exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ì „ì²´ ê²°ê³¼
        summary_data = []
        for name, result in self.results.items():
            summary_data.append({
                "ì•Œê³ ë¦¬ì¦˜": name,
                "ìœ í˜•": "ML" if result.get("type") == "ml" else "Traditional",
                "Top-1 ì •í™•ë„(%)": result["top1_accuracy"],
                "Top-3 ì •í™•ë„(%)": result["top3_accuracy"],
                "Top-5 ì •í™•ë„(%)": result["top5_accuracy"],
                "í‰ê·  ì‹¤í–‰ì‹œê°„(ms)": result["avg_execution_time"],
                "í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ìˆ˜": result["total"]
            })
        
        summary_df = pd.DataFrame(summary_data)
        summary_file = output_dir / f"benchmark_{timestamp}.csv"
        summary_df.to_csv(summary_file, index=False, encoding="utf-8-sig")
        print(f"ğŸ’¾ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ì €ì¥: {summary_file}\n")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("\n" + "="*80)
    print("ğŸ¯ í†µí•© ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸")
    print("   ëª¨ë“  ì¶”ì²œ ì•Œê³ ë¦¬ì¦˜ ì„±ëŠ¥ ë¹„êµ")
    print("="*80 + "\n")
    
    benchmark = BenchmarkTest()
    
    # 1. ë°ì´í„° ë¡œë“œ
    benchmark.load_data()
    
    # 2. í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
    try:
        benchmark.load_test_data("data/test_data.csv")
    except Exception as e:
        print("\nâŒ í…ŒìŠ¤íŠ¸ ì¢…ë£Œ: í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì¤€ë¹„í•´ì£¼ì„¸ìš”.")
        return
    
    # 3. ML ëª¨ë¸ í•™ìŠµ
    try:
        benchmark.initialize_ml()
    except Exception as e:
        print(f"\nâš ï¸ ML ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
        print("ML ì•Œê³ ë¦¬ì¦˜ì„ ì œì™¸í•˜ê³  í…ŒìŠ¤íŠ¸ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.\n")
    
    # 4. ëª¨ë“  ì•Œê³ ë¦¬ì¦˜ í…ŒìŠ¤íŠ¸
    benchmark.run_all_tests()
    
    print("\nâœ… ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("="*80 + "\n")


if __name__ == "__main__":
    main()