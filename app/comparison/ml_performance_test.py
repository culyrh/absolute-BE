"""
ML ì•Œê³ ë¦¬ì¦˜ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
app/comparison/ml_performance_test.py

ì‚¬ìš©ë²•:
    python -m app.comparison.ml_performance_test
"""

import pandas as pd
import numpy as np
from typing import Dict
from datetime import datetime
import time
from pathlib import Path

from app.services.ml_location_recommender import MLLocationRecommender


class MLPerformanceTest:
    """ML ì•Œê³ ë¦¬ì¦˜ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.recommender = MLLocationRecommender()
        self.train_data = None
        self.test_data = None
        self.results = {}
        
    def load_and_train(self):
        """ML ëª¨ë¸ í•™ìŠµ"""
        print("ğŸš€ ML ëª¨ë¸ í•™ìŠµ ì¤‘...")
        
        start_time = time.time()
        accuracy = self.recommender.train()
        train_time = time.time() - start_time
        
        print(f"âœ… í•™ìŠµ ì™„ë£Œ: ì •í™•ë„ {accuracy:.3f}, ì†Œìš” ì‹œê°„ {train_time:.2f}ì´ˆ")
        
        return accuracy, train_time
    
    def generate_test_data(self) -> pd.DataFrame:
        """ì¦ê°• í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±"""
        print("\nğŸ”¬ í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± ì¤‘...")
        
        train_df = self.recommender._load_train_df()
        self.train_data = train_df
        
        usage_types = train_df["ëŒ€ë¶„ë¥˜"].unique()
        
        # ê°„ë‹¨í•˜ê²Œ ê¶Œì—­ ì¶”ì¶œ
        regions = []
        if "ì£¼ì†Œ" in train_df.columns:
            for addr in train_df["ì£¼ì†Œ"]:
                if isinstance(addr, str):
                    parts = addr.split()
                    if len(parts) > 0:
                        region = parts[0]
                        if region not in regions:
                            regions.append(region)
        
        if len(regions) == 0:
            regions = ["ì„œìš¸", "ë¶€ì‚°", "ëŒ€êµ¬", "ì¸ì²œ", "ê´‘ì£¼"]
        
        test_samples = []
        
        for usage_type in usage_types:
            subset = train_df[train_df["ëŒ€ë¶„ë¥˜"] == usage_type]
            
            if len(subset) > 0:
                medians = {}
                for col in self.recommender.FEATURE_COLS:
                    if col in subset.columns:
                        medians[col] = subset[col].median()
                
                for region in regions[:5]:
                    for i in range(3):
                        sample = {
                            "ëŒ€ë¶„ë¥˜": usage_type,
                            "ê¶Œì—­": region,
                            "test_id": f"{usage_type}_{region}_{i+1}"
                        }
                        
                        for col in self.recommender.FEATURE_COLS:
                            if col in medians:
                                noise = np.random.uniform(-0.1, 0.1)
                                base_value = medians[col]
                                sample[col] = max(0, base_value * (1 + noise))
                        
                        test_samples.append(sample)
        
        self.test_data = pd.DataFrame(test_samples)
        print(f"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± ì™„ë£Œ: {len(self.test_data)}ê°œ")
        
        return self.test_data
    
    def run_test(self) -> Dict:
        """ML ì•Œê³ ë¦¬ì¦˜ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        print(f"\nâ±ï¸  ML (Random Forest) í…ŒìŠ¤íŠ¸ ì¤‘...")
        
        results = {
            "algorithm": "ML (Random Forest)",
            "top1_correct": 0,
            "top3_correct": 0,
            "top5_correct": 0,
            "total": len(self.test_data),
            "execution_times": [],
            "usage_type_accuracy": {}
        }
        
        for idx, row in self.test_data.iterrows():
            true_label = row["ëŒ€ë¶„ë¥˜"]
            
            start_time = time.time()
            try:
                predictions = self.recommender._predict_from_row(row, top_n=5)
                execution_time = time.time() - start_time
                results["execution_times"].append(execution_time)
                
                if len(predictions) > 0:
                    # Top-1
                    if predictions[0]["category"] == true_label:
                        results["top1_correct"] += 1
                    
                    # Top-3
                    top3_types = [p["category"] for p in predictions[:3]]
                    if true_label in top3_types:
                        results["top3_correct"] += 1
                    
                    # Top-5
                    if len(predictions) >= 5:
                        top5_types = [p["category"] for p in predictions[:5]]
                        if true_label in top5_types:
                            results["top5_correct"] += 1
                    
                    # ëŒ€ë¶„ë¥˜ë³„
                    if true_label not in results["usage_type_accuracy"]:
                        results["usage_type_accuracy"][true_label] = {"correct": 0, "total": 0}
                    
                    results["usage_type_accuracy"][true_label]["total"] += 1
                    if predictions[0]["category"] == true_label:
                        results["usage_type_accuracy"][true_label]["correct"] += 1
                        
            except Exception as e:
                print(f"  âŒ ì˜¤ë¥˜: {str(e)}")
                results["execution_times"].append(0)
        
        results["top1_accuracy"] = (results["top1_correct"] / results["total"]) * 100
        results["top3_accuracy"] = (results["top3_correct"] / results["total"]) * 100
        results["top5_accuracy"] = (results["top5_correct"] / results["total"]) * 100
        results["avg_execution_time"] = np.mean(results["execution_times"]) * 1000
        
        print(f"   âœ… Top-1 ì •í™•ë„: {results['top1_accuracy']:.2f}%")
        print(f"   âœ… Top-3 ì •í™•ë„: {results['top3_accuracy']:.2f}%")
        print(f"   âœ… Top-5 ì •í™•ë„: {results['top5_accuracy']:.2f}%")
        print(f"   âš¡ í‰ê·  ì‹¤í–‰ì‹œê°„: {results['avg_execution_time']:.2f} ms")
        
        self.results = results
        return results
    
    def save_results(self):
        """ê²°ê³¼ ì €ì¥"""
        output_dir = Path("test_results")
        output_dir.mkdir(exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ì „ì²´ ê²°ê³¼
        summary_data = {
            "ì•Œê³ ë¦¬ì¦˜": self.results["algorithm"],
            "Top-1 ì •í™•ë„(%)": self.results["top1_accuracy"],
            "Top-3 ì •í™•ë„(%)": self.results["top3_accuracy"],
            "Top-5 ì •í™•ë„(%)": self.results["top5_accuracy"],
            "í‰ê·  ì‹¤í–‰ì‹œê°„(ms)": self.results["avg_execution_time"],
            "í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ìˆ˜": self.results["total"]
        }
        
        summary_df = pd.DataFrame([summary_data])
        summary_file = output_dir / f"ml_summary_{timestamp}.csv"
        summary_df.to_csv(summary_file, index=False, encoding="utf-8-sig")
        print(f"\nğŸ’¾ ML ê²°ê³¼ ì €ì¥: {summary_file}")
        
        # ëŒ€ë¶„ë¥˜ë³„ ê²°ê³¼
        usage_data = []
        for usage_type, acc in self.results["usage_type_accuracy"].items():
            usage_data.append({
                "ëŒ€ë¶„ë¥˜": usage_type,
                "ì •í™•ë„(%)": (acc["correct"] / acc["total"]) * 100 if acc["total"] > 0 else 0,
                "ì •í™• ìˆ˜": acc["correct"],
                "ì „ì²´ ìˆ˜": acc["total"]
            })
        
        if usage_data:
            usage_df = pd.DataFrame(usage_data)
            usage_file = output_dir / f"ml_usage_type_{timestamp}.csv"
            usage_df.to_csv(usage_file, index=False, encoding="utf-8-sig")
            print(f"ğŸ’¾ ëŒ€ë¶„ë¥˜ë³„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ\n")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("="*70)
    print("ğŸš€ ML ì•Œê³ ë¦¬ì¦˜ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸")
    print("="*70 + "\n")
    
    test = MLPerformanceTest()
    
    test.load_and_train()
    test.generate_test_data()
    test.run_test()
    test.save_results()
    
    print("\nâœ… ML ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")


if __name__ == "__main__":
    main()